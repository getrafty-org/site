---
layout: layout.jsx
title: "Async I/O"
date: 2025-09-10
---


# Async IO

When you look at servers, it feels like they're doing all sorts of smart stuff. But if you step back, most of the time they're just... moving bytes around. Read some data, write some data, wait for more. It's kind of boring, but also, that's the core of everything.

I keep coming back to IO because no matter what kind of system we try to build, it's always there underneath. Frameworks usually hide the details - which is convenient - but I kind of want to poke at it myself. Like, how exactly do those bytes get packed up and shipped across the wire?

## Terminology

"IO" can mean a lot of different things - reading from disk, sending bytes over the network, or even talking to hardware through [DMA](https://en.wikipedia.org/wiki/Direct_memory_access). Since we're focused on distributed systems, the piece we care about most is network IO.
Network devices are accessible via [sockets](https://en.wikipedia.org/wiki/Berkeley_sockets). A socket is basically the OS handing you a handle and saying: *‚Äúuse this to send bytes out, or I‚Äôll poke you when new bytes show up.‚Äù*. Every major operating system has this concept. 

Now, sockets aren't all the same. They can work in blocking [mode](https://man7.org/linux/man-pages/man7/socket.7.html), where a thread waits until data shows up, or in non-blocking mode, where you tell the OS ‚Äúlet me know when this is ready.‚Äù Blocking is easy to use but doesn't scale if you've got lots of connections - threads just end up sitting idle. Non-blocking avoids that by handing control back to you until there's actual work to do.

On Linux, this is achieved via [`epoll`](https://man7.org/linux/man-pages/man7/epoll.7.html) which gives us a way to monitor many sockets at once and only act on the ones that are ready. Libraries like [Asio](https://think-async.com/Asio/index.html) for C++ or [Tokio](https://tokio.rs/) for Rust use it internally. 


## Events, events are everywhere

At the bottom of it, our toy distributed system would have to talk to other machines via some sort of [RPC](http://dist-prog-book.com/chapter/1/rpc.html).
Sure, we could just grab an off-the-shelf RPC library and call it a day... but what's the fun then, right? In this post I want to poke right at the heart of efficient IO.

### Interface

For this task, we won't be creating a fully functional event loop. Instead, we'll just focus on notification mechanism. Let's call it `EventWatcher`. It allows to `watch` on a file descriptor and call a `IWatchCallback` callback whenever file descriptor is ready to reading or writing.

```cpp
class EventWatcher {
public:
    void watch(int fd, WatchFlag flag, IWatchCallbackPtr cb);
    void unwatch(int fd, WatchFlag flag);
    void unwatchAll();
};
```

The callback is called once fd (e.g. socket) is ready for I/O. 

```cpp
class IWatchCallback {
public:
    virtual void run(int fd) = 0;
};
```

In the `/tasks/async-io` directory you'll find a half-baked implementation of `EventWatcher`. Feel free to tear it apart and change things however you like - but the main missing pieces are already called out with blocks like this:

```cpp
// ==== YOUR CODE: @0000 ====
...
// ==== END YOUR CODE ====
```

### Flexing, multiplexing

The man page for [`man`](https://man7.org/linux/man-pages/man7/epoll.7.html) is great. It tells you what calls to use, all the ways you can shoot yourself in the leg, and even has code samples for the loop. 
That‚Äôs basically what `EventWatcher::waitLoop` is supposed to be doing.

### Callbacks

So what happens after `epoll_wait` hands us a batch of ready file descriptors (maybe)? We grab each one, find its callback, and run it. But where? Inside the event loop thread itself, or offload to some thread pool?

For pure IO, throwing it to another core usually doesn‚Äôt help and often just slows things down with extra context switching. But once we step into request handling, it‚Äôs a different story. 
A typical server besides just reading and write bytes also parses the request, maybe talks to a database, maybe does some business logic, maybe kicks off more IO before replying. That‚Äôs the kind of work where offloading to a pool can actually pay off.

### Handle watch / unwatch

One gotcha: it's possible the `epoll_wait` will hang forever if no events come in. That means newly added fds won‚Äôt be picked up until the current wait cycle ends which, well, may not happen.
In the pasts I used timeouts for that kind of stuff, but recently discovered [self pipe trick](https://cr.yp.to/docs/selfpipe.html).

### üß† Task

Your task is to implement `waitLoop` method of the [`EventWatcher`](https://github.com/getrafty-org/getrafty/blob/main/tasks/rpc-io/event_watcher.hpp) class using [`epoll_wait`](https://man7.org/linux/man-pages/man2/epoll_wait.2.html#top_of_page).

### Testing

Tests are located in [`event_watcher_test.cpp`](https://github.com/getrafty-org/getrafty/blob/main/tasks/rpc-io/event_watcher_test.cpp).

<comp.Oops/>







